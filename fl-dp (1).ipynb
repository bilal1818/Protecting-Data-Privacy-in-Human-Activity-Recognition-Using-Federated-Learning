{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelnel":{"display_name":"Python 3","language":"python","name":"python3"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":793070,"sourceType":"datasetVersion","datasetId":226}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Federated Learning with Differential Privacy + Compression - HAR\n\nThis notebook implements **FedAvg with Differential Privacy and Communication Compression**.\n\n**Experiment Configuration:**\n- Dataset: UCI HAR (Human Activity Recognition)\n- Training Mode: Federated Learning (FedAvg) + Differential Privacy\n- **Privacy Budget (ε)**: [20, 50, 80] (lower = more privacy)\n- **Non-IID (α)**: [0.1, 0.3, 1.0]\n- **Compression**: With/Without (Top-K 10%)\n- **Seeds**: [42, 123, 456]\n- **Total Runs**: 54 (3 α × 3 ε × 3 seeds × 2 compression modes)\n- Rounds: 100\n- Clients: 10 (5 per round)\n\n**Dataset Source:** https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones\n\n**Compatible with:** Local and Kaggle","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup","metadata":{}},{"cell_type":"code","source":"import os\n\nIS_KAGGLE = os.path.exists('/kaggle/input')\nDATA_PATH = '/kaggle/input/human-activity-recognition-with-smartphones' if IS_KAGGLE else '.'\nprint(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\nprint(f\"Data path: {DATA_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:35.599160Z","iopub.execute_input":"2025-11-02T14:30:35.599460Z","iopub.status.idle":"2025-11-02T14:30:35.609309Z","shell.execute_reply.started":"2025-11-02T14:30:35.599435Z","shell.execute_reply":"2025-11-02T14:30:35.608401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport random\nimport json\nfrom datetime import datetime\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(f\"PyTorch: {torch.__version__}, NumPy: {np.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:37.338445Z","iopub.execute_input":"2025-11-02T14:30:37.339079Z","iopub.status.idle":"2025-11-02T14:30:43.198875Z","shell.execute_reply.started":"2025-11-02T14:30:37.339052Z","shell.execute_reply":"2025-11-02T14:30:43.197965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:43.200272Z","iopub.execute_input":"2025-11-02T14:30:43.200647Z","iopub.status.idle":"2025-11-02T14:30:43.209310Z","shell.execute_reply.started":"2025-11-02T14:30:43.200625Z","shell.execute_reply":"2025-11-02T14:30:43.208272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Loading with Non-IID Partitioning","metadata":{}},{"cell_type":"code","source":"class UCIHARDataLoader:\n   def __init__(self, data_path='.'):\n       self.data_path = data_path\n       self.label_encoder = LabelEncoder()\n       self.activity_labels = None\n\n   def load_data(self):\n       train_df = pd.read_csv(os.path.join(self.data_path, 'train.csv'))\n       test_df = pd.read_csv(os.path.join(self.data_path, 'test.csv'))\n\n       X_train = train_df.drop(['Activity', 'subject'], axis=1).values\n       y_train = train_df['Activity'].values\n       X_test = test_df.drop(['Activity', 'subject'], axis=1).values\n       y_test = test_df['Activity'].values\n\n       self.label_encoder.fit(np.concatenate([y_train, y_test]))\n       y_train = self.label_encoder.transform(y_train)\n       y_test = self.label_encoder.transform(y_test)\n\n       self.activity_labels = {i: label for i, label in enumerate(self.label_encoder.classes_)}\n       return X_train, y_train, X_test, y_test\n\n   def partition_data_dirichlet(self, X, y, num_clients, alpha, seed=42):\n       np.random.seed(seed)\n       num_classes = len(np.unique(y))\n       client_indices = {i: [] for i in range(num_clients)}\n\n       class_indices = {k: [] for k in range(num_classes)}\n       for idx, label in enumerate(y):\n           class_indices[label].append(idx)\n\n       for k in range(num_classes):\n           np.random.shuffle(class_indices[k])\n           proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n           proportions = proportions / proportions.sum()\n           proportions = (np.cumsum(proportions) * len(class_indices[k])).astype(int)[:-1]\n           splits = np.split(class_indices[k], proportions)\n           for client_id, split in enumerate(splits):\n               client_indices[client_id].extend(split)\n\n       for client_id in range(num_clients):\n           np.random.shuffle(client_indices[client_id])\n\n       return client_indices\n\n   def get_federated_dataloaders(self, num_clients, alpha, batch_size=64, seed=42):\n       X_train, y_train, X_test, y_test = self.load_data()\n       client_indices = self.partition_data_dirichlet(X_train, y_train, num_clients, alpha, seed)\n\n       client_loaders = []\n       for client_id in range(num_clients):\n           indices = client_indices[client_id]\n           X_client = torch.FloatTensor(X_train[indices])\n           y_client = torch.LongTensor(y_train[indices])\n           client_dataset = TensorDataset(X_client, y_client)\n           client_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n           client_loaders.append(client_loader)\n\n       test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n       test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n       return client_loaders, test_loader, X_train.shape[1], len(self.activity_labels)\n\ndata_loader = UCIHARDataLoader(DATA_PATH)\nX_train, y_train, X_test, y_test = data_loader.load_data()\nprint(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}, Features: {X_train.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:43.210349Z","iopub.execute_input":"2025-11-02T14:30:43.210641Z","iopub.status.idle":"2025-11-02T14:30:45.223804Z","shell.execute_reply.started":"2025-11-02T14:30:43.210621Z","shell.execute_reply":"2025-11-02T14:30:45.223048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Model Architecture","metadata":{}},{"cell_type":"code","source":"class HARModel(nn.Module):\n    def __init__(self, input_dim, num_classes, hidden_dims=[256, 128, 64], dropout=0.5):\n        super(HARModel, self).__init__()\n        layers = []\n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, num_classes))\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.225554Z","iopub.execute_input":"2025-11-02T14:30:45.225818Z","iopub.status.idle":"2025-11-02T14:30:45.232114Z","shell.execute_reply.started":"2025-11-02T14:30:45.225798Z","shell.execute_reply":"2025-11-02T14:30:45.231232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Differential Privacy Mechanism","metadata":{}},{"cell_type":"code","source":"class DifferentialPrivacy:\n    \"\"\"\n    Implements Gaussian Mechanism for Differential Privacy\n    \"\"\"\n    def __init__(self, epsilon, delta=1e-5, sensitivity=1.0):\n        \"\"\"\n        Args:\n            epsilon: Privacy budget (lower = more privacy)\n            delta: Probability of privacy breach\n            sensitivity: L2 sensitivity of the query (gradient clipping norm)\n        \"\"\"\n        self.epsilon = epsilon\n        self.delta = delta\n        self.sensitivity = sensitivity\n        self.sigma = self._calculate_noise_scale()\n    \n    def _calculate_noise_scale(self):\n        \"\"\"\n        Calculate Gaussian noise scale using the Gaussian mechanism\n        σ = (sensitivity * sqrt(2 * ln(1.25/δ))) / ε\n        \"\"\"\n        return (self.sensitivity * np.sqrt(2 * np.log(1.25 / self.delta))) / self.epsilon\n    \n    def add_noise(self, model_state_dict):\n        \"\"\"\n        Add Gaussian noise to model parameters\n        \"\"\"\n        noisy_state = {}\n        for key, param in model_state_dict.items():\n            if 'weight' in key or 'bias' in key:\n                # Add Gaussian noise\n                noise = torch.randn_like(param) * self.sigma\n                noisy_state[key] = param + noise\n            else:\n                # Don't add noise to batch norm statistics\n                noisy_state[key] = param\n        return noisy_state\n    \n    def clip_gradients(self, model, max_norm):\n        \"\"\"\n        Clip gradients to bound sensitivity\n        \"\"\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n# Test DP mechanism\ndp = DifferentialPrivacy(epsilon=50, delta=1e-5, sensitivity=1.0)\nprint(f\"\\nDP Configuration:\")\nprint(f\"  Epsilon (ε): {dp.epsilon}\")\nprint(f\"  Delta (δ): {dp.delta}\")\nprint(f\"  Noise scale (σ): {dp.sigma:.4f}\")\nprint(f\"  Lower ε = More Privacy, Higher Noise\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.233025Z","iopub.execute_input":"2025-11-02T14:30:45.233347Z","iopub.status.idle":"2025-11-02T14:30:45.250876Z","shell.execute_reply.started":"2025-11-02T14:30:45.233320Z","shell.execute_reply":"2025-11-02T14:30:45.249800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Communication Compression","metadata":{}},{"cell_type":"code","source":"class ModelCompressor:\n    def __init__(self, compression_ratio=0.1):\n        self.compression_ratio = compression_ratio\n    \n    def compress_model(self, model_state_dict):\n        compressed_state = {}\n        for key, param in model_state_dict.items():\n            if 'weight' in key or 'bias' in key:\n                flat_param = param.flatten()\n                k = max(1, int(flat_param.numel() * self.compression_ratio))\n                values, indices = torch.topk(torch.abs(flat_param), k)\n                compressed_param = torch.zeros_like(flat_param)\n                compressed_param[indices] = flat_param[indices]\n                compressed_state[key] = compressed_param.reshape(param.shape)\n            else:\n                compressed_state[key] = param\n        return compressed_state\n    \n    def calculate_compression_stats(self, original_state, compressed_state):\n        original_bytes = 0\n        compressed_bytes = 0\n        \n        for key in original_state.keys():\n            if 'weight' in key or 'bias' in key:\n                original_bytes += original_state[key].numel() * 4\n                non_zero = torch.count_nonzero(compressed_state[key])\n                compressed_bytes += non_zero.item() * 8  # value + index\n        \n        compression_ratio = compressed_bytes / original_bytes if original_bytes > 0 else 0\n        return {\n            'original_bytes': original_bytes,\n            'compressed_bytes': compressed_bytes,\n            'compression_ratio': compression_ratio,\n            'savings_percent': (1 - compression_ratio) * 100\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.251878Z","iopub.execute_input":"2025-11-02T14:30:45.252206Z","iopub.status.idle":"2025-11-02T14:30:45.271505Z","shell.execute_reply.started":"2025-11-02T14:30:45.252185Z","shell.execute_reply":"2025-11-02T14:30:45.270593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. FedAvg + DP + Compression","metadata":{}},{"cell_type":"code","source":"def train_client(model, train_loader, criterion, optimizer, device, clip_norm=None, epochs=1):\n    model.train()\n    for epoch in range(epochs):\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Gradient clipping for DP\n            if clip_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n            \n            optimizer.step()\n    return model.state_dict()\n\ndef aggregate_models(global_model, client_models, client_weights):\n    global_dict = global_model.state_dict()\n    for key in global_dict.keys():\n        global_dict[key] = torch.stack(\n            [client_models[i][key].float() * client_weights[i] for i in range(len(client_models))], \n            dim=0\n        ).sum(dim=0)\n    global_model.load_state_dict(global_dict)\n    return global_model\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    test_loss = running_loss / len(all_labels)\n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n    return test_loss, test_acc, test_f1, all_preds, all_labels\n\ndef federated_training_dp(client_loaders, test_loader, input_dim, num_classes, \n                         num_rounds, clients_per_round, learning_rate, weight_decay,\n                         epsilon, delta, clip_norm, \n                         use_compression, compression_ratio,\n                         device, seed=42, verbose=True):\n    set_seed(seed)\n    \n    global_model = HARModel(input_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    dp_mechanism = DifferentialPrivacy(epsilon=epsilon, delta=delta, sensitivity=clip_norm)\n    compressor = ModelCompressor(compression_ratio) if use_compression else None\n    \n    history = {\n        'test_loss': [],\n        'test_acc': [],\n        'test_f1': [],\n        'communication_bytes': []\n    }\n    \n    best_acc = 0.0\n    total_communication_bytes = 0\n    \n    for round_num in range(num_rounds):\n        np.random.seed(seed + round_num)\n        selected_clients = np.random.choice(len(client_loaders), clients_per_round, replace=False)\n        \n        client_models = []\n        client_weights = []\n        round_bytes = 0\n        \n        for client_id in selected_clients:\n            client_model = HARModel(input_dim, num_classes).to(device)\n            client_model.load_state_dict(global_model.state_dict())\n            \n            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n            \n            # Train with gradient clipping\n            client_state = train_client(client_model, client_loaders[client_id], \n                                       criterion, optimizer, device, \n                                       clip_norm=clip_norm, epochs=1)\n            \n            # Add DP noise\n            client_state = dp_mechanism.add_noise(client_state)\n            \n            # Apply compression if enabled\n            if use_compression:\n                compressed_state = compressor.compress_model(client_state)\n                stats = compressor.calculate_compression_stats(client_state, compressed_state)\n                round_bytes += stats['compressed_bytes']\n                client_state = compressed_state\n            else:\n                for param in client_state.values():\n                    round_bytes += param.numel() * 4\n            \n            client_models.append(client_state)\n            client_weights.append(len(client_loaders[client_id].dataset))\n        \n        total_communication_bytes += round_bytes\n        \n        # Aggregate\n        total_size = sum(client_weights)\n        client_weights = [w / total_size for w in client_weights]\n        global_model = aggregate_models(global_model, client_models, client_weights)\n        \n        # Evaluate\n        test_loss, test_acc, test_f1, _, _ = evaluate_model(global_model, test_loader, criterion, device)\n        \n        history['test_loss'].append(test_loss)\n        history['test_acc'].append(test_acc)\n        history['test_f1'].append(test_f1)\n        history['communication_bytes'].append(total_communication_bytes)\n        \n        if test_acc > best_acc:\n            best_acc = test_acc\n        \n        if verbose and (round_num + 1) % 10 == 0:\n            comm_mb = total_communication_bytes / (1024 * 1024)\n            print(f'Round [{round_num+1:3d}/{num_rounds}] '\n                  f'Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Comm: {comm_mb:.2f} MB')\n    \n    test_loss, test_acc, test_f1, predictions, true_labels = evaluate_model(global_model, test_loader, criterion, device)\n    \n    return history, best_acc, test_loss, test_acc, test_f1, predictions, true_labels, total_communication_bytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.272382Z","iopub.execute_input":"2025-11-02T14:30:45.272684Z","iopub.status.idle":"2025-11-02T14:30:45.296001Z","shell.execute_reply.started":"2025-11-02T14:30:45.272654Z","shell.execute_reply":"2025-11-02T14:30:45.295085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Experiment Configuration","metadata":{}},{"cell_type":"code","source":"EXPERIMENT_CONFIG = {\n    'experiment_type': 'fedavg_dp_with_compression',\n    'dataset': 'UCI_HAR',\n    'alpha_values': [0.1, 0.3, 1.0],\n    'epsilon_values': [20, 50, 80],\n    'seeds': [42, 123, 456],\n    'compression_modes': [False, True],\n    'delta': 1e-5,\n    'clip_norm': 1.0,\n    'compression_ratio': 0.1,\n    'num_clients': 10,\n    'clients_per_round': 5,\n    'num_rounds': 100,\n    'batch_size': 64,\n    'learning_rate': 0.001,\n    'weight_decay': 1e-4\n}\n\nprint(\"Experiment Configuration:\")\nprint(\"=\"*60)\nprint(json.dumps(EXPERIMENT_CONFIG, indent=2))\nprint(\"=\"*60)\ntotal_exp = (len(EXPERIMENT_CONFIG['alpha_values']) * \n             len(EXPERIMENT_CONFIG['epsilon_values']) * \n             len(EXPERIMENT_CONFIG['seeds']) * \n             len(EXPERIMENT_CONFIG['compression_modes']))\nprint(f\"\\nTotal Experiments: {total_exp}\")\nprint(f\"  Without Compression: 27 (3 α × 3 ε × 3 seeds)\")\nprint(f\"  With Compression: 27 (3 α × 3 ε × 3 seeds)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.296801Z","iopub.execute_input":"2025-11-02T14:30:45.297111Z","iopub.status.idle":"2025-11-02T14:30:45.317042Z","shell.execute_reply.started":"2025-11-02T14:30:45.297080Z","shell.execute_reply":"2025-11-02T14:30:45.316097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Run All Experiments","metadata":{}},{"cell_type":"code","source":"all_results = []\nexperiment_num = 0\n\nfor use_compression in EXPERIMENT_CONFIG['compression_modes']:\n    for alpha in EXPERIMENT_CONFIG['alpha_values']:\n        for epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n            for seed in EXPERIMENT_CONFIG['seeds']:\n                experiment_num += 1\n                comp_str = \"WITH\" if use_compression else \"WITHOUT\"\n                print(f\"\\n{'='*80}\")\n                print(f\" Exp {experiment_num}/54: α={alpha}, ε={epsilon}, Seed={seed}, {comp_str} Compression \")\n                print(f\"{'='*80}\\n\")\n                \n                set_seed(seed)\n                \n                data_loader = UCIHARDataLoader(DATA_PATH)\n                client_loaders, test_loader, input_dim, num_classes = data_loader.get_federated_dataloaders(\n                    num_clients=EXPERIMENT_CONFIG['num_clients'],\n                    alpha=alpha,\n                    batch_size=EXPERIMENT_CONFIG['batch_size'],\n                    seed=seed\n                )\n                \n                history, best_acc, test_loss, test_acc, test_f1, predictions, true_labels, total_comm = federated_training_dp(\n                    client_loaders=client_loaders,\n                    test_loader=test_loader,\n                    input_dim=input_dim,\n                    num_classes=num_classes,\n                    num_rounds=EXPERIMENT_CONFIG['num_rounds'],\n                    clients_per_round=EXPERIMENT_CONFIG['clients_per_round'],\n                    learning_rate=EXPERIMENT_CONFIG['learning_rate'],\n                    weight_decay=EXPERIMENT_CONFIG['weight_decay'],\n                    epsilon=epsilon,\n                    delta=EXPERIMENT_CONFIG['delta'],\n                    clip_norm=EXPERIMENT_CONFIG['clip_norm'],\n                    use_compression=use_compression,\n                    compression_ratio=EXPERIMENT_CONFIG['compression_ratio'],\n                    device=device,\n                    seed=seed,\n                    verbose=True\n                )\n                \n                result = {\n                    'alpha': alpha,\n                    'epsilon': epsilon,\n                    'seed': seed,\n                    'compression': use_compression,\n                    'test_accuracy': test_acc,\n                    'test_f1': test_f1,\n                    'test_loss': test_loss,\n                    'best_accuracy': best_acc,\n                    'history': history,\n                    'predictions': predictions,\n                    'true_labels': true_labels,\n                    'total_communication_bytes': total_comm\n                }\n                all_results.append(result)\n                \n                comm_mb = total_comm / (1024 * 1024)\n                print(f\"\\n{'-'*80}\")\n                print(f\"Results: Acc={test_acc:.4f}, F1={test_f1:.4f}, Comm={comm_mb:.2f} MB\")\n                print(f\"{'-'*80}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\" ALL 54 EXPERIMENTS COMPLETED SUCCESSFULLY \")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T14:30:45.318068Z","iopub.execute_input":"2025-11-02T14:30:45.318471Z","iopub.status.idle":"2025-11-02T15:11:39.265244Z","shell.execute_reply.started":"2025-11-02T14:30:45.318442Z","shell.execute_reply":"2025-11-02T15:11:39.264366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Results Analysis","metadata":{}},{"cell_type":"code","source":"# Organize results\nresults_by_config = {}\nfor alpha in EXPERIMENT_CONFIG['alpha_values']:\n    for epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n        for comp in [False, True]:\n            key = (alpha, epsilon, comp)\n            results_by_config[key] = [r for r in all_results \n                                      if r['alpha'] == alpha and r['epsilon'] == epsilon and r['compression'] == comp]\n\nprint(\"\\n\" + \"=\"*80)\nprint(\" FL + DP + COMPRESSION ABLATION - SUMMARY \")\nprint(\"=\"*80)\n\n# Summary by epsilon (privacy level)\nfor epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n    print(f\"\\n{'='*80}\")\n    print(f\" Privacy Budget: ε = {epsilon} (Lower ε = More Privacy) \")\n    print(f\"{'='*80}\")\n    \n    for alpha in EXPERIMENT_CONFIG['alpha_values']:\n        print(f\"\\n  Alpha = {alpha}:\")\n        \n        for comp in [False, True]:\n            results = results_by_config[(alpha, epsilon, comp)]\n            accs = [r['test_accuracy'] for r in results]\n            comms = [r['total_communication_bytes'] / (1024**2) for r in results]\n            \n            comp_str = \"WITH Compression\" if comp else \"WITHOUT Compression\"\n            print(f\"    {comp_str}:\")\n            print(f\"      Accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n            print(f\"      Comm (MB): {np.mean(comms):.2f} ± {np.std(comms):.2f}\")\n\n# Create detailed results table\nresults_df = pd.DataFrame([{\n    'Alpha': r['alpha'],\n    'Epsilon': r['epsilon'],\n    'Seed': r['seed'],\n    'Compression': 'Yes' if r['compression'] else 'No',\n    'Accuracy': f\"{r['test_accuracy']:.4f}\",\n    'F1': f\"{r['test_f1']:.4f}\",\n    'Comm (MB)': f\"{r['total_communication_bytes']/(1024**2):.2f}\"\n} for r in all_results])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Sample Results (First 10):\")\nprint(results_df.head(10).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:39.267711Z","iopub.execute_input":"2025-11-02T15:11:39.267995Z","iopub.status.idle":"2025-11-02T15:11:39.288693Z","shell.execute_reply.started":"2025-11-02T15:11:39.267974Z","shell.execute_reply":"2025-11-02T15:11:39.287944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Comprehensive Visualizations","metadata":{}},{"cell_type":"markdown","source":"### 11.1 Privacy-Utility Trade-off","metadata":{}},{"cell_type":"code","source":"# Privacy-Utility trade-off: Epsilon vs Accuracy\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i, alpha in enumerate(EXPERIMENT_CONFIG['alpha_values']):\n    for comp in [False, True]:\n        epsilons = []\n        mean_accs = []\n        std_accs = []\n        \n        for epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n            results = results_by_config[(alpha, epsilon, comp)]\n            accs = [r['test_accuracy'] for r in results]\n            epsilons.append(epsilon)\n            mean_accs.append(np.mean(accs))\n            std_accs.append(np.std(accs))\n        \n        label = 'With Compression' if comp else 'No Compression'\n        marker = '^' if comp else 'o'\n        axes[i].errorbar(epsilons, mean_accs, yerr=std_accs, label=label, \n                        marker=marker, markersize=8, linewidth=2, capsize=5, alpha=0.8)\n    \n    axes[i].set_xlabel('Privacy Budget (ε)', fontsize=11)\n    axes[i].set_ylabel('Test Accuracy', fontsize=11)\n    axes[i].set_title(f'Alpha = {alpha}', fontsize=12, fontweight='bold')\n    axes[i].legend(fontsize=10)\n    axes[i].grid(True, alpha=0.3)\n    axes[i].invert_xaxis()  # Lower epsilon (more privacy) on right\n\nplt.suptitle('Privacy-Utility Trade-off (Lower ε = More Privacy)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('privacy_utility_tradeoff.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:39.289431Z","iopub.execute_input":"2025-11-02T15:11:39.289673Z","iopub.status.idle":"2025-11-02T15:11:41.113274Z","shell.execute_reply.started":"2025-11-02T15:11:39.289654Z","shell.execute_reply":"2025-11-02T15:11:41.112141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11.2 Communication Cost by Privacy Level","metadata":{}},{"cell_type":"code","source":"# Communication cost comparison\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i, epsilon in enumerate(EXPERIMENT_CONFIG['epsilon_values']):\n    alpha_labels = [f'α={alpha}' for alpha in EXPERIMENT_CONFIG['alpha_values']]\n    \n    no_comp_comms = [np.mean([r['total_communication_bytes']/(1024**2) \n                              for r in results_by_config[(alpha, epsilon, False)]]) \n                     for alpha in EXPERIMENT_CONFIG['alpha_values']]\n    with_comp_comms = [np.mean([r['total_communication_bytes']/(1024**2) \n                                for r in results_by_config[(alpha, epsilon, True)]]) \n                       for alpha in EXPERIMENT_CONFIG['alpha_values']]\n    \n    x = np.arange(len(alpha_labels))\n    width = 0.35\n    \n    axes[i].bar(x - width/2, no_comp_comms, width, label='No Compression', alpha=0.8, color='steelblue')\n    axes[i].bar(x + width/2, with_comp_comms, width, label='With Compression', alpha=0.8, color='coral')\n    \n    axes[i].set_xlabel('Non-IID Level', fontsize=11)\n    axes[i].set_ylabel('Communication (MB)', fontsize=11)\n    axes[i].set_title(f'ε = {epsilon}', fontsize=12, fontweight='bold')\n    axes[i].set_xticks(x)\n    axes[i].set_xticklabels(alpha_labels)\n    axes[i].legend(fontsize=10)\n    axes[i].grid(True, alpha=0.3, axis='y')\n\nplt.suptitle('Communication Cost by Privacy Level', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('communication_by_privacy.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:41.114217Z","iopub.execute_input":"2025-11-02T15:11:41.114518Z","iopub.status.idle":"2025-11-02T15:11:42.782458Z","shell.execute_reply.started":"2025-11-02T15:11:41.114493Z","shell.execute_reply":"2025-11-02T15:11:42.781602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11.3 Accuracy Heatmaps","metadata":{}},{"cell_type":"code","source":"# Heatmaps: Epsilon vs Alpha for accuracy\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor idx, comp in enumerate([False, True]):\n    # Prepare heatmap data\n    heatmap_data = np.zeros((len(EXPERIMENT_CONFIG['epsilon_values']), \n                             len(EXPERIMENT_CONFIG['alpha_values'])))\n    \n    for i, epsilon in enumerate(EXPERIMENT_CONFIG['epsilon_values']):\n        for j, alpha in enumerate(EXPERIMENT_CONFIG['alpha_values']):\n            results = results_by_config[(alpha, epsilon, comp)]\n            heatmap_data[i, j] = np.mean([r['test_accuracy'] for r in results])\n    \n    sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', \n                xticklabels=[f'α={a}' for a in EXPERIMENT_CONFIG['alpha_values']],\n                yticklabels=[f'ε={e}' for e in EXPERIMENT_CONFIG['epsilon_values']],\n                ax=axes[idx], vmin=0.8, vmax=1.0, cbar_kws={'label': 'Accuracy'})\n    \n    title = 'With Compression' if comp else 'Without Compression'\n    axes[idx].set_title(f'Accuracy: {title}', fontsize=13, fontweight='bold')\n    axes[idx].set_xlabel('Non-IID Level', fontsize=11)\n    axes[idx].set_ylabel('Privacy Budget', fontsize=11)\n\nplt.suptitle('Accuracy Heatmap: Privacy vs Data Heterogeneity', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('accuracy_heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:42.783186Z","iopub.execute_input":"2025-11-02T15:11:42.783405Z","iopub.status.idle":"2025-11-02T15:11:44.129702Z","shell.execute_reply.started":"2025-11-02T15:11:42.783388Z","shell.execute_reply":"2025-11-02T15:11:44.128786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11.4 Training Curves Comparison","metadata":{}},{"cell_type":"code","source":"# Training curves for different epsilon values (alpha=0.3, first seed)\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nalpha = 0.3  # Middle alpha value\nseed = 42    # First seed\n\nfor i, epsilon in enumerate(EXPERIMENT_CONFIG['epsilon_values']):\n    # Get results for this epsilon\n    no_comp = [r for r in all_results if r['alpha']==alpha and r['epsilon']==epsilon \n               and r['seed']==seed and r['compression']==False][0]\n    with_comp = [r for r in all_results if r['alpha']==alpha and r['epsilon']==epsilon \n                 and r['seed']==seed and r['compression']==True][0]\n    \n    rounds = np.arange(len(no_comp['history']['test_acc']))\n    \n    # Accuracy\n    axes[0, i].plot(rounds, no_comp['history']['test_acc'], label='No Compression', linewidth=2)\n    axes[0, i].plot(rounds, with_comp['history']['test_acc'], label='With Compression', linewidth=2)\n    axes[0, i].set_xlabel('Round', fontsize=10)\n    axes[0, i].set_ylabel('Test Accuracy', fontsize=10)\n    axes[0, i].set_title(f'ε = {epsilon}', fontsize=11, fontweight='bold')\n    axes[0, i].legend(fontsize=9)\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Communication\n    axes[1, i].plot(rounds, np.array(no_comp['history']['communication_bytes'])/(1024**2), \n                   label='No Compression', linewidth=2)\n    axes[1, i].plot(rounds, np.array(with_comp['history']['communication_bytes'])/(1024**2), \n                   label='With Compression', linewidth=2)\n    axes[1, i].set_xlabel('Round', fontsize=10)\n    axes[1, i].set_ylabel('Cumulative Comm (MB)', fontsize=10)\n    axes[1, i].legend(fontsize=9)\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.suptitle(f'Training Curves (α={alpha}, Seed={seed})', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('training_curves_dp.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:44.130749Z","iopub.execute_input":"2025-11-02T15:11:44.131290Z","iopub.status.idle":"2025-11-02T15:11:47.335719Z","shell.execute_reply.started":"2025-11-02T15:11:44.131257Z","shell.execute_reply":"2025-11-02T15:11:47.334759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11.5 Compression Savings Analysis","metadata":{}},{"cell_type":"code","source":"# Calculate compression savings for each configuration\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i, alpha in enumerate(EXPERIMENT_CONFIG['alpha_values']):\n    epsilons = []\n    savings = []\n    acc_diffs = []\n    \n    for epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n        no_comp_comm = np.mean([r['total_communication_bytes'] \n                                for r in results_by_config[(alpha, epsilon, False)]])\n        with_comp_comm = np.mean([r['total_communication_bytes'] \n                                  for r in results_by_config[(alpha, epsilon, True)]])\n        \n        no_comp_acc = np.mean([r['test_accuracy'] \n                               for r in results_by_config[(alpha, epsilon, False)]])\n        with_comp_acc = np.mean([r['test_accuracy'] \n                                 for r in results_by_config[(alpha, epsilon, True)]])\n        \n        epsilons.append(epsilon)\n        savings.append((1 - with_comp_comm / no_comp_comm) * 100)\n        acc_diffs.append((with_comp_acc - no_comp_acc) * 100)\n    \n    # Savings\n    ax1 = axes[i]\n    color = 'tab:blue'\n    ax1.bar(epsilons, savings, alpha=0.7, color=color, label='Comm Savings')\n    ax1.set_xlabel('Privacy Budget (ε)', fontsize=11)\n    ax1.set_ylabel('Communication Savings (%)', fontsize=11, color=color)\n    ax1.tick_params(axis='y', labelcolor=color)\n    ax1.set_title(f'Alpha = {alpha}', fontsize=12, fontweight='bold')\n    ax1.grid(True, alpha=0.3, axis='y')\n    \n    # Accuracy difference\n    ax2 = ax1.twinx()\n    color = 'tab:red'\n    ax2.plot(epsilons, acc_diffs, marker='o', color=color, linewidth=2, \n            markersize=8, label='Acc Difference')\n    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    ax2.set_ylabel('Accuracy Difference (%)', fontsize=11, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n\nplt.suptitle('Compression Impact: Savings vs Accuracy', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('compression_impact.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:47.336699Z","iopub.execute_input":"2025-11-02T15:11:47.337043Z","iopub.status.idle":"2025-11-02T15:11:49.487375Z","shell.execute_reply.started":"2025-11-02T15:11:47.337015Z","shell.execute_reply":"2025-11-02T15:11:49.486495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11.6 Confusion Matrix (Best Configuration)","metadata":{}},{"cell_type":"code","source":"# Find best configuration overall\nbest_result = max(all_results, key=lambda x: x['test_accuracy'])\n\nprint(f\"\\nBest Configuration:\")\nprint(f\"  Alpha: {best_result['alpha']}\")\nprint(f\"  Epsilon: {best_result['epsilon']}\")\nprint(f\"  Seed: {best_result['seed']}\")\nprint(f\"  Compression: {'Yes' if best_result['compression'] else 'No'}\")\nprint(f\"  Accuracy: {best_result['test_accuracy']:.4f}\")\nprint(f\"  F1 Score: {best_result['test_f1']:.4f}\")\n\n# Plot confusion matrix\ncm = confusion_matrix(best_result['true_labels'], best_result['predictions'])\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=data_loader.activity_labels.values(),\n            yticklabels=data_loader.activity_labels.values(),\n            cbar_kws={'label': 'Count'})\n\nplt.title(f'Confusion Matrix - Best Config\\n'\n         f'α={best_result[\"alpha\"]}, ε={best_result[\"epsilon\"]}, '\n         f'Comp={\"Yes\" if best_result[\"compression\"] else \"No\"}\\n'\n         f'Acc: {best_result[\"test_accuracy\"]:.4f}',\n         fontsize=13, fontweight='bold')\nplt.ylabel('True Label', fontsize=11)\nplt.xlabel('Predicted Label', fontsize=11)\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('confusion_matrix_best.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:49.488518Z","iopub.execute_input":"2025-11-02T15:11:49.488953Z","iopub.status.idle":"2025-11-02T15:11:50.444635Z","shell.execute_reply.started":"2025-11-02T15:11:49.488908Z","shell.execute_reply":"2025-11-02T15:11:50.443765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12. Save Results","metadata":{}},{"cell_type":"code","source":"os.makedirs('results/fedavg_dp_compression', exist_ok=True)\n\n# Calculate summary statistics\nsummary_by_config = {}\nfor alpha in EXPERIMENT_CONFIG['alpha_values']:\n    for epsilon in EXPERIMENT_CONFIG['epsilon_values']:\n        for comp in [False, True]:\n            key = f\"alpha_{alpha}_epsilon_{epsilon}_comp_{comp}\"\n            results = results_by_config[(alpha, epsilon, comp)]\n            \n            summary_by_config[key] = {\n                'mean_accuracy': float(np.mean([r['test_accuracy'] for r in results])),\n                'std_accuracy': float(np.std([r['test_accuracy'] for r in results])),\n                'mean_f1': float(np.mean([r['test_f1'] for r in results])),\n                'mean_communication_mb': float(np.mean([r['total_communication_bytes']/(1024**2) for r in results]))\n            }\n\noutput = {\n    'experiment_config': EXPERIMENT_CONFIG,\n    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'device': str(device),\n    'summary_by_config': summary_by_config,\n    'best_overall': {\n        'alpha': best_result['alpha'],\n        'epsilon': best_result['epsilon'],\n        'seed': best_result['seed'],\n        'compression': best_result['compression'],\n        'test_accuracy': float(best_result['test_accuracy']),\n        'test_f1': float(best_result['test_f1'])\n    },\n    'runs': [\n        {\n            'alpha': r['alpha'],\n            'epsilon': r['epsilon'],\n            'seed': r['seed'],\n            'compression': r['compression'],\n            'test_accuracy': float(r['test_accuracy']),\n            'test_f1': float(r['test_f1']),\n            'communication_mb': float(r['total_communication_bytes']/(1024**2))\n        }\n        for r in all_results\n    ]\n}\n\nwith open('results/fedavg_dp_compression/results.json', 'w') as f:\n    json.dump(output, f, indent=2)\n\nresults_df.to_csv('results/fedavg_dp_compression/summary.csv', index=False)\n\nprint(\"\\nResults saved:\")\nprint(\"=\"*60)\nprint(\"  - results/fedavg_dp_compression/results.json\")\nprint(\"  - results/fedavg_dp_compression/summary.csv\")\nprint(\"\\nVisualizations saved:\")\nprint(\"  - privacy_utility_tradeoff.png\")\nprint(\"  - communication_by_privacy.png\")\nprint(\"  - accuracy_heatmap.png\")\nprint(\"  - training_curves_dp.png\")\nprint(\"  - compression_impact.png\")\nprint(\"  - confusion_matrix_best.png\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:50.445763Z","iopub.execute_input":"2025-11-02T15:11:50.446112Z","iopub.status.idle":"2025-11-02T15:11:50.466744Z","shell.execute_reply.started":"2025-11-02T15:11:50.446086Z","shell.execute_reply":"2025-11-02T15:11:50.465992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}